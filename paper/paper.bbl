\begin{thebibliography}{10}

\bibitem{SALTON1988513}
Gerard Salton and Christopher Buckley.
\newblock Term-weighting approaches in automatic text retrieval.
\newblock {\em Information Processing \& Management}, 24(5):513--523, 1988.

\bibitem{10.5555/944919.944937}
David~M. Blei, Andrew~Y. Ng, and Michael~I. Jordan.
\newblock Latent dirichlet allocation.
\newblock {\em J. Mach. Learn. Res.}, 3(null):993â€“1022, March 2003.

\bibitem{mikolov2013efficientestimationwordrepresentations}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space, 2013.

\bibitem{pennington-etal-2014-glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock {G}lo{V}e: Global vectors for word representation.
\newblock In Alessandro Moschitti, Bo~Pang, and Walter Daelemans, editors, {\em Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}, pages 1532--1543, Doha, Qatar, October 2014. Association for Computational Linguistics.

\bibitem{peters-etal-2018-deep}
Matthew~E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock In Marilyn Walker, Heng Ji, and Amanda Stent, editors, {\em Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 2227--2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{liu2019robertarobustlyoptimizedbert}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem{sun2019ernieenhancedrepresentationknowledge}
Yu~Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu.
\newblock Ernie: Enhanced representation through knowledge integration, 2019.

\bibitem{diao-etal-2020-zen}
Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang.
\newblock Zen: Pre-training chinese text encoder enhanced by n-gram representations.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 4729--4740, Online, November 2020.

\bibitem{DXSJSZ2021}
Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, and Tong Zhang.
\newblock Taming pre-trained language models with n-gram representations for low-resource domain adaptation.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3336--3349, Online, August 2021. Association for Computational Linguistics.

\end{thebibliography}
