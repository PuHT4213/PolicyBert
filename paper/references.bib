@article{SALTON1988513,
title = {Term-weighting approaches in automatic text retrieval},
journal = {Information Processing \& Management},
volume = {24},
number = {5},
pages = {513-523},
year = {1988},
issn = {0306-4573},
doi = {https://doi.org/10.1016/0306-4573(88)90021-0},
url = {https://www.sciencedirect.com/science/article/pii/0306457388900210},
author = {Gerard Salton and Christopher Buckley},
abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.}
}
@article{10.5555/944919.944937,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent dirichlet allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}
@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}
@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162/",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543"
}
@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202/",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}
@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{sun2019ernieenhancedrepresentationknowledge,
      title={ERNIE: Enhanced Representation through Knowledge Integration}, 
      author={Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Xuyi Chen and Han Zhang and Xin Tian and Danxiang Zhu and Hao Tian and Hua Wu},
      year={2019},
      eprint={1904.09223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09223}, 
}

@inproceedings{diao-etal-2020-zen,
    title = "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations",
    author = "Diao, Shizhe and Bai, Jiaxin and Song, Yan and Zhang, Tong and Wang, Yonggang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    pages = "4729--4740",
}
@inproceedings{DXSJSZ2021,
    title = "Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation",
    author = "Diao, Shizhe  and
      Xu, Ruijia  and
      Su, Hongjin  and
      Jiang, Yilei  and
      Song, Yan  and
      Zhang, Tong",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.259",
    doi = "10.18653/v1/2021.acl-long.259",
    pages = "3336--3349",
}
@article{Ma2021PTWAPW,
  title={PTWA: Pre-Training with Word Attention for Chinese Named Entity Recognition},
  author={Kaixin Ma and Meiling Liu and Tiejun Zhao and Jiyun Zhou and Yang Yu},
  journal={2021 International Joint Conference on Neural Networks (IJCNN)},
  year={2021},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:237598949}
}
@inproceedings{Li2022ExploitingWS,
  title={Exploiting Word Semantics to Enrich Character Representations of Chinese Pre-trained Models},
  author={Wenbiao Li and Rui Sun and Yunfang Wu},
  booktitle={Natural Language Processing and Chinese Computing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:250493034}
}
@article{He2023PromptBasedWI,
  title={Prompt-Based Word-Level Information Injection BERT for Chinese Named Entity Recognition},
  author={Qiang He and Guowei Chen and Wenchao Song and Pengzhou Zhang},
  journal={Applied Sciences},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257397041}
}
@misc{lakatos2024investigatingperformanceretrievalaugmentedgeneration,
      title={Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems}, 
      author={Robert Lakatos and Peter Pollner and Andras Hajdu and Tamas Joo},
      year={2024},
      eprint={2403.09727},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09727}, 
}
@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}
@misc{langchainchatchat,
    title        = {{langchain-chatchat}},
    author       = {Liu, Qian and Song, Jinke and Huang, Zhiguo and Zhang, Yuxuan and glide-the and liunux4odoo},
    year         = 2024,
    journal      = {GitHub repository},
    publisher    = {GitHub},
    howpublished = {\url{https://github.com/chatchat-space/Langchain-Chatchat}}
}
@inproceedings{he-choi-2021-stem,
    title = "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders",
    author = "He, Han and Choi, Jinho D.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.451",
    pages = "5555--5577",
    abstract = "Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.",
}
@inproceedings{emerson-2005-second,
    title = "The Second International {C}hinese Word Segmentation Bakeoff",
    author = "Emerson, Thomas",
    booktitle = "Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing",
    year = "2005",
    url = "https://aclanthology.org/I05-3017/"
}
@article{10.1017/S135132490400364X,
    author = {Xue, Naiwen and Xia, Fei and Chiou, Fu-dong and Palmer, Marta},
    title = {The Penn Chinese TreeBank: Phrase structure annotation of a large corpus},
    year = {2005},
    issue_date = {June 2005},
    publisher = {Cambridge University Press},
    address = {USA},
    volume = {11},
    number = {2},
    issn = {1351-3249},
    url = {https://doi.org/10.1017/S135132490400364X},
    doi = {10.1017/S135132490400364X},
    abstract = {With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.},
    journal = {Nat. Lang. Eng.},
    month = jun,
    pages = {207–238},
    numpages = {32}
}
@inproceedings{levow-2006-third,
    title = "The Third International {C}hinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition",
    author = "Levow, Gina-Anne",
    booktitle = "Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-0115",
    pages = "108--117",
}
@InProceedings{conneau2018xnli,
  author = "Conneau, Alexis
        and Rinott, Ruty
        and Lample, Guillaume
        and Williams, Adina
        and Bowman, Samuel R.
        and Schwenk, Holger
        and Stoyanov, Veselin",
  title = "XNLI: Evaluating Cross-lingual Sentence Representations",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods
               in Natural Language Processing",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  location = "Brussels, Belgium",
}
@inproceedings{liu-etal-2018-lcqmc,
    title = "{LCQMC}:A Large-scale {C}hinese Question Matching Corpus",
    author = "Liu, Xin  and
      Chen, Qingcai  and
      Deng, Chong  and
      Zeng, Huajun  and
      Chen, Jing  and
      Li, Dongfang  and
      Tang, Buzhou",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1166/",
    pages = "1952--1962",
    abstract = "The lack of large-scale question matching corpora greatly limits the development of matching methods in question answering (QA) system, especially for non-English languages. To ameliorate this situation, in this paper, we introduce a large-scale Chinese question matching corpus (named LCQMC), which is released to the public1. LCQMC is more general than paraphrase corpus as it focuses on intent matching rather than paraphrase. How to collect a large number of question pairs in variant linguistic forms, which may present the same intent, is the key point for such corpus construction. In this paper, we first use a search engine to collect large-scale question pairs related to high-frequency words from various domains, then filter irrelevant pairs by the Wasserstein distance, and finally recruit three annotators to manually check the left pairs. After this process, a question matching corpus that contains 260,068 question pairs is constructed. In order to verify the LCQMC corpus, we split it into three parts, i.e., a training set containing 238,766 question pairs, a development set with 8,802 question pairs, and a test set with 12,500 question pairs, and test several well-known sentence matching methods on it. The experimental results not only demonstrate the good quality of LCQMC but also provide solid baseline performance for further researches on this corpus."
}
@misc{sun2016thuctc,
  author = {M. Sun and J. Li and Z. Guo and Z. Yu and Y. Zheng and X. Si and Z. Liu},
  title = {THUCTC: An Efficient Chinese Text Classifier},
  year = {2016},
  howpublished = {\url{https://github.com/thunlp/THUCTC}},
  note = {GitHub Repository}
}
@misc{yfwt-wr77-20,
doi = {10.21227/yfwt-wr77},
url = {https://dx.doi.org/10.21227/yfwt-wr77},
author = {Songbo Tan},
publisher = {IEEE Dataport},
title = {ChnSentiCorp},
year = {2020}
}
@article{Cui_2021,
   title={Pre-Training With Whole Word Masking for Chinese BERT},
   volume={29},
   ISSN={2329-9304},
   url={http://dx.doi.org/10.1109/TASLP.2021.3124365},
   DOI={10.1109/taslp.2021.3124365},
   journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
   year={2021},
   pages={3504–3514} 
}
@misc{sun2019ernie20continualpretraining,
      title={ERNIE 2.0: A Continual Pre-training Framework for Language Understanding}, 
      author={Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Hao Tian and Hua Wu and Haifeng Wang},
      year={2019},
      eprint={1907.12412},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.12412}, 
}
@misc{wei2021nezhaneuralcontextualizedrepresentation,
      title={NEZHA: Neural Contextualized Representation for Chinese Language Understanding}, 
      author={Junqiu Wei and Xiaozhe Ren and Xiaoguang Li and Wenyong Huang and Yi Liao and Yasheng Wang and Jiashu Lin and Xin Jiang and Xiao Chen and Qun Liu},
      year={2021},
      eprint={1909.00204},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.00204}, 
}
@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bojanowski2017enrichingwordvectorssubword,
      title={Enriching Word Vectors with Subword Information}, 
      author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
      year={2017},
      eprint={1607.04606},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1607.04606}, 
}
@misc{peters2018deepcontextualizedwordrepresentations,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1802.05365}, 
}
@inproceedings{Cui_2020,
   title={Revisiting Pre-Trained Models for Chinese Natural Language Processing},
   url={http://dx.doi.org/10.18653/v1/2020.findings-emnlp.58},
   DOI={10.18653/v1/2020.findings-emnlp.58},
   booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
   publisher={Association for Computational Linguistics},
   author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Wang, Shijin and Hu, Guoping},
   year={2020} 
}
@misc{xiao2021erniegrampretrainingexplicitlyngram,
      title={ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding}, 
      author={Dongling Xiao and Yu-Kun Li and Han Zhang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},
      year={2021},
      eprint={2010.12148},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.12148}, 
}
@misc{liang2023characterwordbothrevisiting,
      title={Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models}, 
      author={Xinnian Liang and Zefan Zhou and Hui Huang and Shuangzhi Wu and Tong Xiao and Muyun Yang and Zhoujun Li and Chao Bian},
      year={2023},
      eprint={2303.10893},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10893}, 
}
@article{Lee_2019,
   title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
   volume={36},
   ISSN={1367-4811},
   url={http://dx.doi.org/10.1093/bioinformatics/btz682},
   DOI={10.1093/bioinformatics/btz682},
   number={4},
   journal={Bioinformatics},
   publisher={Oxford University Press (OUP)},
   author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
   editor={Wren, Jonathan},
   year={2019},
   month=sep, pages={1234–1240} 
}
@misc{zhang2021smedbertknowledgeenhancedpretrainedlanguage,
      title={SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining}, 
      author={Taolin Zhang and Zerui Cai and Chengyu Wang and Minghui Qiu and Bite Yang and Xiaofeng He},
      year={2021},
      eprint={2108.08983},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.08983}, 
}
@article{Liu_2021,
   title={Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning},
   volume={32},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2021.3099165},
   DOI={10.1109/tnnls.2021.3099165},
   number={9},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Liu, Guangyi and Liao, Yinghong and Wang, Fuyu and Zhang, Bin and Zhang, Lu and Liang, Xiaodan and Wan, Xiang and Li, Shaolin and Li, Zhen and Zhang, Shuixing and Cui, Shuguang},
   year={2021},
   month=sep, pages={3786–3797} 
}
@misc{chalkidis2020legalbertmuppetsstraightlaw,
      title={LEGAL-BERT: The Muppets straight out of Law School}, 
      author={Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
      year={2020},
      eprint={2010.02559},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.02559}, 
}
@misc{xiao2021lawformerpretrainedlanguagemodel,
      title={Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents}, 
      author={Chaojun Xiao and Xueyu Hu and Zhiyuan Liu and Cunchao Tu and Maosong Sun},
      year={2021},
      eprint={2105.03887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2105.03887}, 
}
@techreport{zhong2019openclap,
  title={Open Chinese Language Pre-trained Model Zoo},
  author={Zhong, Haoxi and Zhang, Zhengyan and Liu, Zhiyuan and Sun, Maosong},
  year={2019},
  url = "https://github.com/thunlp/openclap",
  institution={THUNLP},
}
@INPROCEEDINGS{9849586,
  author={Yu, Bihui and Deng, Chen and Bu, Liping},
  booktitle={2022 11th International Conference of Information and Communication Technology (ICTech))}, 
  title={Policy Text Classification Algorithm Based on Bert}, 
  year={2022},
  volume={},
  number={},
  pages={488-491},
  keywords={Deep learning;Text categorization;Bit error rate;Semantics;Transformers;Information age;Classification algorithms;deep learning;BERT;policy text;text classification;word vector},
  doi={10.1109/ICTech55460.2022.00103}}